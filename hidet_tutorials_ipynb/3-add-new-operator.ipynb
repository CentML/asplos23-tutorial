{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add New Operator\n",
    "Hidet is designed to be extensible. It is easy to add new operators to Hidet. There are two ways to add and schedule an operator.\n",
    "1. **Rule-based Scheduling** Define the mathematical computation of the operator, and Hidet will automatically schedule the computation into a parallel tensor program with Hidet's rule-based scheduler.\n",
    "2. **Template-based Scheduling** Besides the computation, users can also give the concrete implementation of the operator to achieve better performance for complex operators.  \n",
    "\n",
    "In this tutorial, we will walk through how to define the computation of an operator and schedule it using the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hidet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Operator Computation\n",
    "Each operator takes a list of input tensors and produces a list of output tensors:\n",
    "\n",
    "```\n",
    "inputs: List[Tensor]\n",
    "outputs: List[Tensor] = operator(inputs)\n",
    "```\n",
    "The precise mathematical definition of each operator in Hidet is defined through a domain-specific-language (DSL). In this tutorial, we will show how to define the mathematical definition of a new operator in Hidet using this DSL, which is defined in the [hidet.ir.compute](https://docs.hidet.org/stable/python_api/ir/compute.html#module-hidet.ir.compute) module.\n",
    "\n",
    "### 1.1 Compute Primitives\n",
    "\n",
    "Hidet provides compute primitives to define the mathematical computation of an operator.\n",
    "#### 1.1.1 tensor_input\n",
    "```python\n",
    "tensor_input(name: sttr, dtype: str, shape: List[int])\n",
    "```\n",
    "The [`tensor_input()`](https://docs.hidet.org/stable/python_api/ir/compute.html#hidet.ir.compute.tensor_input) primitive defines a tensor inputby specifying the name hint, scalar data type, and shape of the tensor.\n",
    "\n",
    "<sub>Examples:</sub>\n",
    "```python\n",
    "a = tensor_input('a', dtype='float32', shape=[10, 10])\n",
    "b = tensor_input('b', dtype='float32', shape=[])\n",
    "b = tensor_input('data', dtype='float16', shape=[1, 3, 224, 224])\n",
    "```\n",
    "\n",
    "#### 1.1.2 compute\n",
    "```python\n",
    "compute(name: str, shape: List[int], fcompute: Callable[[Var, ...], Expr])\n",
    "```\n",
    "\n",
    "The [`compute()`](https://docs.hidet.org/stable/python_api/ir/compute.html#hidet.ir.compute.compute) primitive defines a tensor by specifying\n",
    "* the name of the tensor, just a hint for what the tensor represents,\n",
    "* the shape of the tensor, and\n",
    "* a function that maps an index to the expression that computes the value of the tensor at that index.\n",
    "\n",
    "The computation of each element of the tensor is *independent* with each other and can be computed in parallel.\n",
    "\n",
    "<sub>Semantics:</sub>\n",
    "```python\n",
    "# compute primitive\n",
    "out = compute(\n",
    "    name='hint_name',\n",
    "    shape=[n1, n2, ..., nk],\n",
    "    fcompute=lambda i1, i2, ..., ik: f(i1, i2, ..., ik)\n",
    ")\n",
    "\n",
    "# semantics\n",
    "for i1 in range(n1):\n",
    "  for i2 in range(n2):\n",
    "    ...\n",
    "      for ik in range(nk):\n",
    "        out[i1, i2, ..., ik] = f(i1, i2, ..., ik)\n",
    "```\n",
    "\n",
    "<sub>Examples:</sub>\n",
    "```python\n",
    "# define an input tensor\n",
    "a = tensor_input('a', dtype='float32', shape=[10, 10])\n",
    "\n",
    "# example 1: slice the first column of a\n",
    "b = compute('slice', shape=[10], fcompute=lambda i: a[i, 0])\n",
    "\n",
    "# example 2: reverse the rows of matrix a\n",
    "c = compute('reverse', shape=[10, 10], fcompute=lambda i, j: a[9 - i, j])\n",
    "\n",
    "# example 3: add 1 to the diagonal elements of a\n",
    "from hidet.ir.expr import if_then_else\n",
    "d = compute(\n",
    "  name='diag_add',\n",
    "  shape=[10, 10],\n",
    "  fcompute=lambda i, j: if_then_else(i == j, then_expr=a[i, j] + 1.0, else_expr=a[i, j])\n",
    ")\n",
    "```\n",
    "\n",
    "#### 1.1.3 reduce\n",
    "```python\n",
    "reduce(shape: List[int], fcompute: Callable[[Var, ...], Expr], reduce_type='sum')\n",
    "```\n",
    "\n",
    "The [`reduce()`](https://docs.hidet.org/stable/python_api/ir/compute.html#hidet.ir.compute.reduce) primitive conducts a reduction operation on a domain with the given shape. It returns a scalar value and can be used in [`compute()`](https://docs.hidet.org/stable/python_api/ir/compute.html#hidet.ir.compute.compute) primitive.\n",
    "\n",
    "<sub>Semantics:</sub>\n",
    "```python\n",
    "# reduce primitive\n",
    "out = reduce(\n",
    "    name='hint_name',\n",
    "    shape=[n1, n2, ..., nk],\n",
    "    fcompute=lambda i1, i2, ..., ik: f(i1, i2, ..., ik)\n",
    "    reduce_type='sum' | 'max' | 'min' | 'avg'\n",
    ")\n",
    "\n",
    "# semantics\n",
    "values = []\n",
    "for i1 in range(n1):\n",
    "  for i2 in range(n2):\n",
    "    ...\n",
    "      for ik in range(nk):\n",
    "        values.append(f(i1, i2, ..., ik))\n",
    "out = reduce_type(values)\n",
    "```\n",
    "\n",
    "<sub>Examples:</sub>\n",
    "```python\n",
    "# define an input tensor\n",
    "a = tensor_input('a', dtype='float32', shape=[10, 10])\n",
    "\n",
    "# example 1: sum all elements of a\n",
    "c = reduce(shape=[10, 10], fcompute=lambda i, j: a[i, j], reduce_type='sum')\n",
    "\n",
    "# example 2: sum the first column of a\n",
    "d = reduce(shape=[10], fcompute=lambda i: a[i, 0], reduce_type='sum')\n",
    "\n",
    "# example 3: matrix multiplication\n",
    "b = tensor_input('b', dtype='float32', shape=[10, 10])\n",
    "e = compute(\n",
    "    name='e',\n",
    "    shape=[10, 10],\n",
    "    fcompute=lambda i, j: reduce(\n",
    "        shape=[10],\n",
    "        fcompute=lambda k: a[i, k] * b[k, j],\n",
    "        reduce_type='sum'\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define a Computation Task\n",
    "The computation of each operator can be described as a directed acyclic graph (DAG). The DAG is composed of tensor nodes. Both [`tensor_input()`](https://docs.hidet.org/stable/python_api/ir/compute.html#hidet.ir.compute.tensor_input) and [`compute()`](https://docs.hidet.org/stable/python_api/ir/compute.html#hidet.ir.compute.compute) primitives create tensor nodes. The edges of the DAG are the dependencies between the tensor nodes. Such a DAG is stored in a [`Task`](https://docs.hidet.org/stable/python_api/ir/task.html#hidet.ir.task.Task) object.\n",
    "```python\n",
    "class Task(name: str, inputs: List[TensorNode], outputs: List[TensorNode])\n",
    "```\n",
    "Each task has a name, a list of inputs, and a list of outputs, correspongding to the inputs and outputs of the operator. The following example shows how to create a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task(\n",
      "  name: task\n",
      "  parameters: \n",
      "    a: tensor(float32, [10])\n",
      "    b: tensor(float32, [10])\n",
      "    d: tensor(float32, [10])\n",
      "    e: tensor(float32, [10])\n",
      "  inputs: [a, b]\n",
      "  outputs: [d, e]\n",
      "  computations: \n",
      "    b: tensor(float32, [10])\n",
      "    e: float32[10] where e[v] = (a[v] + b[v])\n",
      "    a: tensor(float32, [10])\n",
      "    c: float32[10] where c[v_1] = (a[v_1] + v_1)\n",
      "    d: float32[10] where d[v_2] = c[(9 - v_2)]\n",
      "  attributes: {}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def demo_task():\n",
    "    from hidet.ir.compute import tensor_input, compute\n",
    "    from hidet.ir.task import Task\n",
    "\n",
    "    # define the computation DAG through the compute primitives\n",
    "    a = tensor_input('a', dtype='float32', shape=[10])\n",
    "    b = tensor_input('b', dtype='float32', shape=[10])\n",
    "    c = compute('c', [10], lambda i: a[i] + i)\n",
    "    d = compute('d', [10], lambda i: c[9 - i])\n",
    "    e = compute('e', [10], lambda i: a[i] + b[i])\n",
    "\n",
    "    # create a task object\n",
    "    task = Task(name='task', inputs=[a, b], outputs=[d, e])\n",
    "    print(task)\n",
    "\n",
    "\n",
    "demo_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, there are 5 tensor nodes, where node `a` and `b` are inputs and node `d` and `e`. The computation of node `c` depends on the computation of node `a`. Node `d` depends on node `c`, and node `e` depends on both nodes `a` and `b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build and Run a Task\n",
    "We provide a driver function [`hidet.driver.build_task()`](https://docs.hidet.org/stable/python_api/driver.html#hidet.driver.build_task) to build a task into callable function. The [`build_task()`](https://docs.hidet.org/stable/python_api/driver.html#hidet.driver.build_task) function does the following steps to lower the task into a callable function:\n",
    "1. Dispatch the task to a **scheduler** according to the target device and task.\n",
    "2. The scheduler lowers the task into a tensor program, defined with [`IRModule`](https://docs.hidet.org/stable/python_api/ir/func.html#hidet.ir.func.IRModule).\n",
    "3. Lower and optimize the `IRModule`.\n",
    "4. Code generation that translates the IRModule into the target source code (e.g., `source.cu`).\n",
    "5. Call compiler (e.g., `nvcc`) to compile the source code into a dynamic library (i.e., `lib.so`).\n",
    "6. Load the dynamic library and wrap it to [`CompiledFunction`](https://docs.hidet.org/stable/python_api/runtime/index.html#hidet.runtime.CompiledFunction) that can be directly called.\n",
    "\n",
    "We can define the following function to build and run a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from hidet.ir.task import Task\n",
    "\n",
    "\n",
    "def run_task(task: Task, inputs: List[hidet.Tensor], outputs: List[hidet.Tensor]):\n",
    "    \"\"\"Run given task and print inputs and outputs\"\"\"\n",
    "    from hidet.runtime import CompiledFunction\n",
    "\n",
    "    # build the task\n",
    "    func: CompiledFunction = hidet.driver.build_task(task, target_device='cpu')\n",
    "    params = inputs + outputs\n",
    "\n",
    "    # run the compiled task\n",
    "    func(*params)\n",
    "\n",
    "    print('Task:', task.name)\n",
    "    print('Inputs:')\n",
    "    for tensor in inputs:\n",
    "        print(tensor)\n",
    "    print('Output:')\n",
    "    for tensor in outputs:\n",
    "        print(tensor)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows how to 1) define the computation, 2) define the task, and 3) build and run the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling cpu task \u001b[92madd(a=float32[5], b=float32[5])\u001b[0m...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: add\n",
      "Inputs:\n",
      "Tensor(shape=(5,), dtype='float32', device='cpu')\n",
      "[ 0.5170048  -0.8175022  -0.6692999  -0.2707757  -0.36673257]\n",
      "Tensor(shape=(5,), dtype='float32', device='cpu')\n",
      "[-0.8175891   0.4673212  -0.54976064 -1.0559387   0.30592343]\n",
      "Output:\n",
      "Tensor(shape=(5,), dtype='float32', device='cpu')\n",
      "[-0.30058432 -0.350181   -1.2190605  -1.3267144  -0.06080914]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hidet.ir.compute import tensor_input, reduce, compute, arg_reduce, TensorNode\n",
    "\n",
    "def add_example():\n",
    "    a: TensorNode = tensor_input(name='a', dtype='float32', shape=[5])\n",
    "    b: TensorNode = tensor_input(name='b', dtype='float32', shape=[5])\n",
    "    c: TensorNode = compute(name='c', shape=[5], fcompute=lambda i: a[i] + b[i])\n",
    "    task = Task(name='add', inputs=[a, b], outputs=[c])\n",
    "    run_task(task, [hidet.randn([5]), hidet.randn([5])], [hidet.empty([5])])\n",
    "\n",
    "add_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 More Examples\n",
    "We show more examples of using the compute primitives to define operator computation.\n",
    "\n",
    "### 3.1.1 ReduceSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling cpu task \u001b[92mreduce_sum(a=float32[4, 3])\u001b[0m...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: reduce_sum\n",
      "Inputs:\n",
      "Tensor(shape=(4, 3), dtype='float32', device='cpu')\n",
      "[[-0.21989535 -0.6286531  -0.52672076]\n",
      " [ 0.7672621   0.82575005 -1.0160285 ]\n",
      " [ 1.0468827  -0.6883719   0.29560193]\n",
      " [ 0.9096517   0.968135    0.9219604 ]]\n",
      "Output:\n",
      "Tensor(shape=(4,), dtype='float32', device='cpu')\n",
      "[-1.3752692  0.5769836  0.6541128  2.799747 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def reduce_sum_example():\n",
    "    a = tensor_input('a', dtype='float32', shape=[4, 3])\n",
    "    b = compute(\n",
    "        'b',\n",
    "        shape=[4],\n",
    "        fcompute=lambda i: reduce(\n",
    "            shape=[3], fcompute=lambda j: a[i, j], reduce_type='sum'\n",
    "        ),\n",
    "    )\n",
    "    task = Task('reduce_sum', inputs=[a], outputs=[b])\n",
    "    run_task(task, [hidet.randn([4, 3])], [hidet.empty([4])])\n",
    "\n",
    "\n",
    "reduce_sum_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 MatMul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling cpu task \u001b[92mmatmul(a=float32[3, 3], b=float32[3, 3])\u001b[0m...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: matmul\n",
      "Inputs:\n",
      "Tensor(shape=(3, 3), dtype='float32', device='cpu')\n",
      "[[-0.12335024  1.1787595  -1.1675321 ]\n",
      " [ 0.26967815  0.23652509  1.9818728 ]\n",
      " [-1.5455004   0.08275169  0.76819605]]\n",
      "Tensor(shape=(3, 3), dtype='float32', device='cpu')\n",
      "[[ 0.55907583  1.2184253  -0.12690002]\n",
      " [ 0.65917647  0.9455997  -0.7075036 ]\n",
      " [ 0.6402905  -0.6257311   1.5101963 ]]\n",
      "Output:\n",
      "Tensor(shape=(3, 3), dtype='float32', device='cpu')\n",
      "[[-0.03951138  1.6949027  -2.581526  ]\n",
      " [ 1.5756567  -0.6878787   2.7914524 ]\n",
      " [-0.31763536 -2.285511    1.2977037 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def matmul_example():\n",
    "    a = tensor_input('a', dtype='float32', shape=[3, 3])\n",
    "    b = tensor_input('b', dtype='float32', shape=[3, 3])\n",
    "    c = compute(\n",
    "        'c',\n",
    "        shape=[3, 3],\n",
    "        fcompute=lambda i, j: reduce(\n",
    "            shape=[3], fcompute=lambda k: a[i, k] * b[k, j], reduce_type='sum'\n",
    "        ),\n",
    "    )\n",
    "    task = Task('matmul', inputs=[a, b], outputs=[c])\n",
    "    run_task(task, [hidet.randn([3, 3]), hidet.randn([3, 3])], [hidet.empty([3, 3])])\n",
    "\n",
    "\n",
    "matmul_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Add an Operator with Rule-based Scheduling\n",
    "So far, we have learned how to define the computation using compute primitives and wrap it into a [`Task`](https://docs.hidet.org/stable/python_api/ir/task.html#hidet.ir.task.Task). In this section, we will learn how to add an [`Operator`](https://docs.hidet.org/stable/python_api/graph/index.html#hidet.graph.Operator) with the given computation definition, and use hidet's privided rule-based scheduler to automatically schedule the computation into a tensor program.\n",
    "\n",
    "## 4.1 Three steps to define a new operator\n",
    "There are three steps to define a new operator in Hidet.\n",
    "1. Define the computation task class by inheriting [`Task`](https://docs.hidet.org/stable/python_api/ir/task.html#hidet.ir.task.Task).\n",
    "2. Define the operator class by inheriting [`Operator`](https://docs.hidet.org/stable/python_api/graph/index.html#hidet.graph.Operator).\n",
    "3. Define a function to create the operator instance.\n",
    "\n",
    "## 4.2 Batch Matrix Multiplication Example\n",
    "We will take the batch matrix multiplication as an example to illustrate the three steps.\n",
    "\n",
    "### 4.2.1. Define the computation task class\n",
    "We define the computation task class `BatchMatmulTask` by inheriting [`Task`](https://docs.hidet.org/stable/python_api/ir/task.html#hidet.ir.task.Task) class. The `BatchMatmulTask` classâ€™s constructor function takes two arguments, `a` and `b` that are the input tensor nodes of the batch matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hidet.ir.compute import TensorNode, compute, reduce\n",
    "from hidet.ir.task import Task\n",
    "\n",
    "\n",
    "class BatchMatmulTask(Task):\n",
    "    def __init__(self, a: TensorNode, b: TensorNode):\n",
    "        # get the input sizes\n",
    "        batch_size, m_size, k_size = a.const_shape()\n",
    "        batch_size, k_size, n_size = b.const_shape()\n",
    "\n",
    "        # define the computation\n",
    "        c = compute(\n",
    "            name='c',\n",
    "            shape=[batch_size, m_size, n_size],\n",
    "            fcompute=lambda p, i, j: reduce(\n",
    "                shape=[k_size],\n",
    "                fcompute=lambda k: a[p, i, k] * b[p, k, j],\n",
    "                reduce_type='sum',\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # call the parent class constructor to initialize the task\n",
    "        super().__init__(\n",
    "            name='batch_matmul',  # the name of the task\n",
    "            inputs=[a, b],  # the input tensor nodes\n",
    "            outputs=[c],  # the output tensor nodes\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Define the operator class\n",
    "Our next step is to define the operator class `BatchMatmulOp` by inheriting [`Operator`](https://docs.hidet.org/stable/python_api/graph/index.html#hidet.graph.Operator) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hidet.graph import Operator, Tensor\n",
    "from hidet.graph.ops.definitions.utils import input_like\n",
    "\n",
    "\n",
    "class BatchMatmulOp(Operator):\n",
    "    def __init__(self, a: Tensor, b: Tensor):\n",
    "        # call the parent class constructor to initialize the operator\n",
    "        super().__init__(\n",
    "            inputs=[a, b],  # the input tensors\n",
    "            task=BatchMatmulTask(  # the task of the operator\n",
    "                # create tensor nodes (TensorNode) with the same shape and dtype as the tensors (Tensor)\n",
    "                input_like(a, 'a'),\n",
    "                input_like(b, 'b'),\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Define a function to create the operator instance\n",
    "We define a function `batch_matmul` to create the operator instance `BatchMatmulOp` and return the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_matmul(a: Tensor, b: Tensor) -> Tensor:\n",
    "    # get_output(0) returns the first output tensor of the operator\n",
    "    return BatchMatmulOp(a, b).get_output(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4. Use the defined operator\n",
    "The new operator has no difference with the hidet provided operators, as we define hidet operators in the same way. For example, when we optimize the flow graph, this new operator can also fuse surrounding operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling cpu task \u001b[92mbatch_matmul(a=float32[2, 2, 3], b=float32[2, 3, 2])\u001b[0m...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=(2, 2, 3), dtype='float32', device='cpu')\n",
      "[[[-0.1188942  -0.03512365 -1.1575822 ]\n",
      "  [-0.60656774 -0.7368701  -0.81118315]]\n",
      "\n",
      " [[-1.2190988  -1.5492649   0.4305561 ]\n",
      "  [-1.2007065   1.2391245  -1.319195  ]]]\n",
      "Tensor(shape=(2, 3, 2), dtype='float32', device='cpu')\n",
      "[[[-1.4443336   0.86954886]\n",
      "  [-2.3942506   0.97157407]\n",
      "  [-1.4177141   0.6442079 ]]\n",
      "\n",
      " [[ 0.20606913 -1.0127158 ]\n",
      "  [ 0.02667027  1.5317795 ]\n",
      "  [-1.2534379   0.34543064]]]\n",
      "Tensor(shape=(2, 2, 2), dtype='float32', device='cpu')\n",
      "[[[ 1.8969383  -0.8832331 ]\n",
      "  [ 3.7903638  -1.7659348 ]]\n",
      "\n",
      " [[-0.8322132  -0.98980427]\n",
      "  [ 1.4391483   2.6583495 ]]]\n"
     ]
    }
   ],
   "source": [
    "def demo_usage():\n",
    "    a = hidet.randn([2, 2, 3])\n",
    "    b = hidet.randn([2, 3, 2])\n",
    "    c = batch_matmul(a, b)\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "\n",
    "demo_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Two Scheduling Mechanisms\n",
    "We only define the computation of the operator, and leave the scheduling to the rule-based scheduler provided by hidet. We call this method of scheduling as **rule-based scheduling**. Most hidet operators are using the same rule-based scheduler as we used in this example. Our experience shows that the rule-based scheduler can achieve good performance for operators that do not have large amount of reduction. However, for operators like matrix multiplication, convolution, etc., the rule-based scheduler may not be able to achieve the best performance as it does not use shared memory to cache the data loading. Thus, hidet also provides another scheduling mechanism, the **template-based scheduling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Add an Operator with Template-based Scheduling\n",
    "Template-based scheduling allows us to define a tensor program template, and the template will be instantiated for different input shapes and tunable hyper-parameters.\n",
    "\n",
    "## 5.1 Override `implement_cuda()` method\n",
    "The [`Task`](https://docs.hidet.org/stable/python_api/ir/task.html#hidet.ir.task.Task) class have two methods, `implement_cpu()` and `implement_cuda()` that we can override when we define a new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hidet.ir.compute import TensorNode, compute, reduce\n",
    "from hidet.ir.task import Task\n",
    "from hidet.ir.func import IRModule\n",
    "\n",
    "class BatchMatmulFp16Task(Task):\n",
    "    def __init__(self, a: TensorNode, b: TensorNode):\n",
    "        batch_size, m_size, k_size = a.const_shape()\n",
    "        batch_size, k_size, n_size = b.const_shape()\n",
    "        c = compute(\n",
    "            name='c',\n",
    "            shape=[batch_size, m_size, n_size],\n",
    "            fcompute=lambda p, i, j: reduce(\n",
    "                shape=[k_size],\n",
    "                fcompute=lambda k: a[p, i, k] * b[p, k, j],\n",
    "                reduce_type='sum',\n",
    "            ),\n",
    "        )\n",
    "        super().__init__(\n",
    "            name='batch_matmul_fp16',\n",
    "            inputs=[a, b],\n",
    "            outputs=[c],\n",
    "            attributes={\n",
    "                'batch_size': batch_size,\n",
    "                'm_size': m_size,\n",
    "                'n_size': n_size,\n",
    "                'k_size': k_size,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def allow_epilogue(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def implement_cuda(self, working_dir: str) -> IRModule:\n",
    "        # override this method to use template-based scheduling\n",
    "        return batch_matmul_mma_fp16_schedule(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above task definition, we override the `implement_cuda()` method to use template-based scheduling. Inside the `implement_cuda()` method, we call the `batch_matmul_mma_fp16_schedule()` function which we will write to get a tensor program that implements the computation defined in the task.\n",
    "\n",
    "## 5.2. Implement the tensor program\n",
    "We can implement the `batch_matmul_mma_fp16_schedule()` function in the following way. This function is written using Hidet Script, a DSL for writing tensor programs, which we will explore in detail in the next section. Understanding the below code requires knowledge in Hidet Script and efficient CUDA programming. For now, we can skip the details of this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_matmul_mma_fp16_schedule(task: BatchMatmulFp16Task) -> IRModule:\n",
    "    from hidet.lang import f16, spatial, repeat, tensor, attr, grid, printf, cast\n",
    "    from hidet.lang.mapping import repeat, spatial\n",
    "    from hidet.lang.cuda import blockIdx, threadIdx, syncthreads\n",
    "    from hidet.lang.cuda import MmaConfig, mma_sync\n",
    "    from hidet.transforms.tools import add_packed_func\n",
    "\n",
    "    # get the workload size\n",
    "    bs = task.attributes['batch_size']\n",
    "    m_size = task.attributes['m_size']\n",
    "    n_size = task.attributes['n_size']\n",
    "    k_size = task.attributes['k_size']\n",
    "\n",
    "    # define the template hyper-parameters\n",
    "    mma_config = MmaConfig.m16n8k8_f16_f16()\n",
    "    block_m, block_n, block_k = 128, 128, 8\n",
    "    warp_m, warp_n, warp_k = 64, 64, 8\n",
    "    warp_count_m, warp_count_n, warp_count_k = 2, 2, 1\n",
    "    mma_m, mma_n, mma_k = mma_config.m, mma_config.n, mma_config.k  # 16, 8, 8\n",
    "    mma_count_m, mma_count_n, mma_count = 4, 8, 1\n",
    "    threads = warp_count_m * warp_count_n * warp_count_k * 32\n",
    "\n",
    "    # define the tensor program\n",
    "    with hidet.script_module() as module:\n",
    "\n",
    "        @hidet.script\n",
    "        def load_regs_a(\n",
    "            smem_a: f16[block_m, block_k], regs_a: f16[4, mma_config.a_elements]\n",
    "        ):\n",
    "            \"\"\"Load A registers from shared memory.\"\"\"\n",
    "            warp_id, lane_id = threadIdx.x / 32, threadIdx.x % 32\n",
    "            for wi, wj, wk in spatial(warp_count_m, warp_count_n, warp_count_k).on(\n",
    "                warp_id\n",
    "            ):\n",
    "                for mi in range(mma_count_m):\n",
    "                    p = 0\n",
    "                    for i, k in mma_config.a_load_map.on(lane_id):\n",
    "                        regs_a[mi, p] = smem_a[\n",
    "                            wi * warp_m + mi * mma_m + i, wk * warp_k + k\n",
    "                        ]\n",
    "                        p += 1\n",
    "\n",
    "        @hidet.script\n",
    "        def load_regs_b(\n",
    "            smem_b: f16[block_k, block_n], regs_b: f16[8, mma_config.b_elements]\n",
    "        ):\n",
    "            \"\"\"Load B registers from shared memory.\"\"\"\n",
    "            warp_id, lane_id = threadIdx.x / 32, threadIdx.x % 32\n",
    "            for wi, wj, wk in spatial(warp_count_m, warp_count_n, warp_count_k).on(\n",
    "                warp_id\n",
    "            ):\n",
    "                for mj in range(mma_count_n):\n",
    "                    p = 0\n",
    "                    for k, j in mma_config.b_load_map.on(lane_id):\n",
    "                        regs_b[mj, p] = smem_b[\n",
    "                            wk * warp_k + k, wj * warp_n + mj * mma_n + j\n",
    "                        ]\n",
    "                        p += 1\n",
    "\n",
    "        @hidet.script\n",
    "        def warp_mma(\n",
    "            regs_a: f16[4, mma_config.a_elements],\n",
    "            regs_b: f16[8, mma_config.b_elements],\n",
    "            regs_c: f16[4, 8, mma_config.c_elements],\n",
    "        ):\n",
    "            \"\"\"Perform warp-level matrix multiplication.\"\"\"\n",
    "            for mi, mj in repeat(mma_count_m, mma_count_n).on(0):\n",
    "                mma_sync(mma_config, ~regs_a[mi, 0], ~regs_b[mj, 0], ~regs_c[mi, mj, 0])\n",
    "\n",
    "        @hidet.script\n",
    "        def store_c(regs_c: f16[4, 8, mma_config.c_elements], c: f16[bs, m_size, n_size]):\n",
    "            \"\"\"Store C registers to global memory.\"\"\"\n",
    "            warp_id, lane_id = threadIdx.x / 32, threadIdx.x % 32\n",
    "            offset_m, offset_n = blockIdx.x * block_m, blockIdx.y * block_n\n",
    "            gmem_c = c[blockIdx.z, offset_m:, offset_n:]\n",
    "            for k_round in range(warp_count_k):\n",
    "                for wi, wj, wk in spatial(warp_count_m, warp_count_n, warp_count_k).on(\n",
    "                    warp_id\n",
    "                ):\n",
    "                    if wk == k_round:\n",
    "                        for mi, mj in repeat(mma_count_m, mma_count_n).on(0):\n",
    "                            p = 0\n",
    "                            for i, j in mma_config.c_store_map.on(lane_id):\n",
    "                                gmem_c.write(\n",
    "                                    [\n",
    "                                        wi * warp_m + mi * mma_m + i,\n",
    "                                        wj * warp_n + mj * mma_n + j,\n",
    "                                    ],\n",
    "                                    regs_c[mi, mj, p],\n",
    "                                    protected=True,\n",
    "                                )\n",
    "                                p += 1\n",
    "\n",
    "        @hidet.script\n",
    "        def batch_matmul_kernel(\n",
    "            a: f16[bs, m_size, k_size],\n",
    "            b: f16[bs, k_size, n_size],\n",
    "            c: f16[bs, m_size, n_size],\n",
    "        ):\n",
    "            \"\"\"Batch matrix multiplication kernel.\"\"\"\n",
    "            attr.cuda_grid_dim = (\n",
    "                (m_size + block_m - 1) // block_m,\n",
    "                (n_size + block_n - 1) // block_n,\n",
    "                bs,\n",
    "            )\n",
    "            attr.cuda_block_dim = threads\n",
    "            offset_m, offset_n = blockIdx.x * block_m, blockIdx.y * block_n\n",
    "            smem_a = tensor('shared', 'float16', [block_m, block_k])\n",
    "            smem_b = tensor('shared', 'float16', [block_k, block_n])\n",
    "            regs_a = tensor('register', 'float16', [4, mma_config.a_elements])\n",
    "            regs_b = tensor('register', 'float16', [8, mma_config.b_elements])\n",
    "            regs_c = tensor('register', 'float16', [4, 8, mma_config.c_elements])\n",
    "\n",
    "            for i, j, p in grid(4, 8, mma_config.c_elements):\n",
    "                regs_c[i, j, p] = 0.0\n",
    "\n",
    "            for k0 in range((k_size + block_k - 1) // block_k):\n",
    "                offset_k = k0 * block_k\n",
    "                gmem_a = a[blockIdx.z, offset_m:, offset_k:]\n",
    "                gmem_b = b[blockIdx.z, offset_k:, offset_n:]\n",
    "                for i, k in repeat(8, 1).spatial(16, 8).on(threadIdx.x):\n",
    "                    smem_a[i, k] = gmem_a.read([i, k], protected=True)\n",
    "                for k, j in repeat(8, 1).spatial(1, 128).on(threadIdx.x):\n",
    "                    smem_b[k, j] = gmem_b.read([k, j], protected=True)\n",
    "                syncthreads()\n",
    "                load_regs_a(smem_a, regs_a)\n",
    "                load_regs_b(smem_b, regs_b)\n",
    "                warp_mma(regs_a, regs_b, regs_c)\n",
    "                syncthreads()\n",
    "            store_c(regs_c, c)\n",
    "\n",
    "    ir_module = module.ir_module()\n",
    "    # conduct the fusion (when the task has prologue or epilogue) and generate the packed function\n",
    "    # ir_module = fuse_and_pack(ir_module, kernel_func=batch_matmul_kernel, task=task)\n",
    "    add_packed_func(ir_module, func=batch_matmul_kernel, pack_func_name=task.name)\n",
    "    return ir_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Define the operator\n",
    "The remaining part is the same as the rule-based scheduling method to add new operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling cpu task \u001b[92mcast(x=float32[1, 2, 2])\u001b[0m...\n",
      "Compiling cuda task \u001b[92mbatch_matmul_fp16(a=float16[1, 2, 2], b=float16[1, 2, 2], batch_size=1, m_size=2, n_size=2, k_size=2)\u001b[0m...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=(1, 2, 2), dtype='float16', device='cuda:0')\n",
      "[[[0.0829  1.327  ]\n",
      "  [0.12366 1.723  ]]]\n",
      "Tensor(shape=(1, 2, 2), dtype='float16', device='cuda:0')\n",
      "[[[-0.05673  0.521  ]\n",
      "  [-0.934   -0.864  ]]]\n",
      "Tensor(shape=(1, 2, 2), dtype='float16', device='cuda:0')\n",
      "[[[-1.244 -1.104]\n",
      "  [-1.616 -1.424]]]\n"
     ]
    }
   ],
   "source": [
    "from hidet.graph import Operator, Tensor\n",
    "from hidet.graph.ops.definitions.utils import input_like\n",
    "\n",
    "class BatchMatmulFp16Op(Operator):\n",
    "    def __init__(self, a: Tensor, b: Tensor):\n",
    "        assert a.dtype == hidet.float16 and b.dtype == hidet.float16\n",
    "        super().__init__(\n",
    "            inputs=[a, b],\n",
    "            task=BatchMatmulFp16Task(input_like(a, 'a'), input_like(b, 'b')),\n",
    "        )\n",
    "\n",
    "\n",
    "def batch_matmul_fp16(a: Tensor, b: Tensor) -> Tensor:\n",
    "    return BatchMatmulFp16Op(a, b).get_output(0)\n",
    "\n",
    "\n",
    "def demo_usage():\n",
    "    a = hidet.randn([1, 2, 2], dtype='float16', device='cuda')\n",
    "    b = hidet.randn([1, 2, 2], dtype='float16', device='cuda')\n",
    "    c = batch_matmul_fp16(a, b)\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)\n",
    "\n",
    "\n",
    "demo_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summary\n",
    "In this section, we have learned how to add a new operator to Hidet. We first define the computation task of the operator through compute primitives. We then define the operator class and either use rule-based scheduling or write our own schedule template and use template-based scheduling to implement the operator.\n",
    "In the next section, we will learn about Hidet Script, a DSL which allows us to conveniently write our own efficient schedules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
